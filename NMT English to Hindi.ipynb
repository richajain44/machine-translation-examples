{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program on Machine Translation from English to Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data and creating dataframe with English sentences and corresponding hinid sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eword=[]\n",
    "hword=[]\n",
    "with open('C:\\\\Users\\\\richa\\\\Desktop\\\\conversica\\\\data.txt',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line=line.split('\\t')\n",
    "        eword.append(line[0])\n",
    "        hword.append(line[1])\n",
    "\n",
    "lines =pd.DataFrame([eword,hword]).T\n",
    "lines.columns =['eword','hword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines['hword']=lines['hword'].apply(lambda x : x.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### I considered only 30000 lines as going for more lines gave memory error on my machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines =lines[0:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eword</th>\n",
       "      <th>hword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>चले जाओ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>रन !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  eword     hword\n",
       "0   Go.  चले जाओ!\n",
       "1  Run!      रन !"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines.eword = lines.eword.apply(lambda x: x.lower())\n",
    "lines.hword =lines.hword.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines.eword=lines.eword.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "lines.hword=lines.hword.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "lines.eword=lines.eword.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.hword=lines.hword.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines.eword=lines.eword.apply(lambda x: x.translate(remove_digits))\n",
    "lines.hword=lines.hword.apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines.hword = lines.hword.apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eword</th>\n",
       "      <th>hword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>START_ चले जाओ _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run</td>\n",
       "      <td>START_ रन  _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  eword                hword\n",
       "0    go  START_ चले जाओ _END\n",
       "1   run      START_ रन  _END"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### unque english and hindi words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_eng_words=set()\n",
    "for eng in lines.eword:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "all_hindi_words=set()\n",
    "for hin in lines.hword:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hindi_words:\n",
    "            all_hindi_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4517, 5359)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_eng_words), len(all_hindi_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###finding the maximum length of a sentence in english and hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenght_list=[]\n",
    "for l in lines.hword:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "np.max(lenght_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenght_list=[]\n",
    "for l in lines.eword:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "np.max(lenght_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_hindi_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_hindi_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros( (len(lines.eword), 7), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(lines.hword), 17),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(lines.hword), 17, num_decoder_tokens),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(lines.eword, lines.hword)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building keras encoder decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "en_x=  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n",
    "encoder = LSTM(50, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "dex=  Embedding(num_decoder_tokens, embedding_size)\n",
    "\n",
    "final_dex= dex(decoder_inputs)\n",
    "\n",
    "\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex,initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28500 samples, validate on 1500 samples\n",
      "Epoch 1/27\n",
      "28500/28500 [==============================] - 675s 24ms/step - loss: 1.5468 - acc: 0.0723 - val_loss: 1.7391 - val_acc: 0.0811\n",
      "Epoch 2/27\n",
      "28500/28500 [==============================] - 668s 23ms/step - loss: 1.3027 - acc: 0.0978 - val_loss: 1.5694 - val_acc: 0.1178\n",
      "Epoch 3/27\n",
      "28500/28500 [==============================] - 670s 24ms/step - loss: 1.1728 - acc: 0.1240 - val_loss: 1.4630 - val_acc: 0.1410\n",
      "Epoch 4/27\n",
      "28500/28500 [==============================] - 656s 23ms/step - loss: 1.0894 - acc: 0.1391 - val_loss: 1.3936 - val_acc: 0.1512\n",
      "Epoch 5/27\n",
      "28500/28500 [==============================] - 675s 24ms/step - loss: 1.0266 - acc: 0.1504 - val_loss: 1.3355 - val_acc: 0.1635\n",
      "Epoch 6/27\n",
      "28500/28500 [==============================] - 678s 24ms/step - loss: 0.9759 - acc: 0.1593 - val_loss: 1.2935 - val_acc: 0.1696\n",
      "Epoch 7/27\n",
      "28500/28500 [==============================] - 585s 21ms/step - loss: 0.9347 - acc: 0.1672 - val_loss: 1.2670 - val_acc: 0.1747\n",
      "Epoch 8/27\n",
      "28500/28500 [==============================] - 450s 16ms/step - loss: 0.9067 - acc: 0.1736 - val_loss: 1.2504 - val_acc: 0.1766\n",
      "Epoch 9/27\n",
      "28500/28500 [==============================] - 457s 16ms/step - loss: 0.8847 - acc: 0.1799 - val_loss: 1.2450 - val_acc: 0.1817\n",
      "Epoch 10/27\n",
      "28500/28500 [==============================] - 453s 16ms/step - loss: 0.8673 - acc: 0.1849 - val_loss: 1.2423 - val_acc: 0.1842\n",
      "Epoch 11/27\n",
      "28500/28500 [==============================] - 456s 16ms/step - loss: 0.8531 - acc: 0.1894 - val_loss: 1.2198 - val_acc: 0.1868\n",
      "Epoch 12/27\n",
      "28500/28500 [==============================] - 457s 16ms/step - loss: 0.8296 - acc: 0.1939 - val_loss: 1.2170 - val_acc: 0.1872\n",
      "Epoch 13/27\n",
      "28500/28500 [==============================] - 454s 16ms/step - loss: 0.8158 - acc: 0.1976 - val_loss: 1.2126 - val_acc: 0.1895\n",
      "Epoch 14/27\n",
      "28500/28500 [==============================] - 463s 16ms/step - loss: 0.8011 - acc: 0.2014 - val_loss: 1.2074 - val_acc: 0.1882\n",
      "Epoch 15/27\n",
      "28500/28500 [==============================] - 455s 16ms/step - loss: 0.7871 - acc: 0.2051 - val_loss: 1.2120 - val_acc: 0.1884\n",
      "Epoch 16/27\n",
      "28500/28500 [==============================] - 460s 16ms/step - loss: 0.7767 - acc: 0.2082 - val_loss: 1.2096 - val_acc: 0.1923\n",
      "Epoch 17/27\n",
      "28500/28500 [==============================] - 465s 16ms/step - loss: 0.7680 - acc: 0.2113 - val_loss: 1.2085 - val_acc: 0.1922\n",
      "Epoch 18/27\n",
      "28500/28500 [==============================] - 463s 16ms/step - loss: 0.7616 - acc: 0.2138 - val_loss: 1.2081 - val_acc: 0.1937\n",
      "Epoch 19/27\n",
      "28500/28500 [==============================] - 461s 16ms/step - loss: 0.7562 - acc: 0.2156 - val_loss: 1.2085 - val_acc: 0.1969\n",
      "Epoch 20/27\n",
      "28500/28500 [==============================] - 466s 16ms/step - loss: 0.7498 - acc: 0.2177 - val_loss: 1.2148 - val_acc: 0.1952\n",
      "Epoch 21/27\n",
      "28500/28500 [==============================] - 462s 16ms/step - loss: 0.7433 - acc: 0.2195 - val_loss: 1.2130 - val_acc: 0.1940\n",
      "Epoch 22/27\n",
      "28500/28500 [==============================] - 456s 16ms/step - loss: 0.7372 - acc: 0.2215 - val_loss: 1.2124 - val_acc: 0.1980\n",
      "Epoch 23/27\n",
      "28500/28500 [==============================] - 458s 16ms/step - loss: 0.7308 - acc: 0.2232 - val_loss: 1.2147 - val_acc: 0.1964\n",
      "Epoch 24/27\n",
      "28500/28500 [==============================] - 459s 16ms/step - loss: 0.7247 - acc: 0.2247 - val_loss: 1.2159 - val_acc: 0.1975\n",
      "Epoch 25/27\n",
      "28500/28500 [==============================] - 455s 16ms/step - loss: 0.7189 - acc: 0.2263 - val_loss: 1.2135 - val_acc: 0.1980\n",
      "Epoch 26/27\n",
      "28500/28500 [==============================] - 453s 16ms/step - loss: 0.7129 - acc: 0.2277 - val_loss: 1.2154 - val_acc: 0.1978\n",
      "Epoch 27/27\n",
      "28500/28500 [==============================] - 453s 16ms/step - loss: 0.7069 - acc: 0.2289 - val_loss: 1.2119 - val_acc: 0.1974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21f0c0787b8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,batch_size=32,epochs=27,validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, None, 50)          225850    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                [(None, 50), (None, 50),  20200     \n",
      "=================================================================\n",
      "Total params: 246,050\n",
      "Trainable params: 246,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### creating sampling sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "final_dex2= dex(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### function to generate the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 52):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### testing on sample inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 300    stop that\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  उसे मत दो । _END\n",
      "-\n",
      "Input sentence: 200    its here\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  यह यहाँ है _END\n",
      "-\n",
      "Input sentence: 400    you drive\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  आप की ज़रूरत है। _END\n",
      "-\n",
      "Input sentence: 500    did we win\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  क्या हम इसे हैं _END\n",
      "-\n",
      "Input sentence: 600    i cant go\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  मैं नहीं जा सकता हूँ _END\n",
      "-\n",
      "Input sentence: 700    i want you\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  मैं तुम पर भरोसा करता हूं । _END\n",
      "-\n",
      "Input sentence: 1    run\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  _END\n",
      "-\n",
      "Input sentence: 235    lie still\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  चलो भी बस _END\n",
      "-\n",
      "Input sentence: 2000    i got dizzy\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  मुझे कुछ मैं एक आ रहा है । _END\n",
      "-\n",
      "Input sentence: 1000    drop it\n",
      "Name: eword, dtype: object\n",
      "Decoded sentence:  इसे ले लो । _END\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [300,200,400,500, 600, 700, 1, 235, 2000, 1000]:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', lines.eword[seq_index: seq_index + 1])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
